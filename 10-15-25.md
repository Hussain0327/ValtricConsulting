# Valtric Consulting – Daily Log (2025-10-15)

## Overview
- Repository: `~/Downloads/ValtricConsulting/valtric`
- Objective: Stand up an AI-assisted valuation workflow that ingests deal documents, stores embeddings in Supabase/pgvector, and powers `/analyze` responses via DeepSeek + GPT-5 Nano.
- Status: End-to-end flow is working locally (Docker Postgres + Supabase). `/foundry/ingest` writes to both stores, `/analyze` performs adaptive reasoning, and Supabase RPC `match_chunks` feeds evidence back into the agent.

## Backend State
- **Python stack:** FastAPI, SQLAlchemy (async), pgvector, Supabase REST RPC, OpenAI/DeepSeek clients.
- **Key endpoints**
  - `POST /foundry/ingest`: persists `deal + documents + chunks + embeddings`. Requires payload `{"deal": {...}, "documents":[...]}`.
  - `POST /analyze`: fetches deals, runs Supabase vector search, classifies “easy/hard,” orchestrates DeepSeek triage + GPT-5 memo, stores the analysis row.
- **Settings:** `.env` now includes `EMBEDDING_MODEL=text-embedding-3-small`, Supabase URL + service-role key, and the dual-model reasoning config. Replace keys immediately if they leak.
- **Logging:** `/analyze` prints `analysis_classification` and `analyze_pipeline_timings` (DeepSeek/GPT latencies); Supabase search failures are resolved.

## Supabase Integration
- RPC `match_chunks` deployed under `public`, signature `vector(1536)` → returns chunks ordered by Euclidean distance (`<->`). Service-role key must be used in headers (`apikey`/`Authorization`).
- Table alignment: `documents`, `chunks`, `embeddings` in Supabase mirror the local schema (`BIGINT` ids, `vector(1536)` columns). RLS disabled for ingestion tables; service-role granted `ALL`.
- Testing shortcut:
  ```bash
  export SUPABASE_SERVICE_ROLE_KEY='...'
  curl -i "https://wupfktkhluphvpfyqwcm.supabase.co/rest/v1/rpc/match_chunks" \
    -H "apikey: $SUPABASE_SERVICE_ROLE_KEY" \
    -H "Authorization: Bearer $SUPABASE_SERVICE_ROLE_KEY" \
    -H "Content-Type: application/json" \
    -H "Prefer: params=single-object" \
    --data '{"query_embedding":"[0,0,0,...1536 values...]", "match_count":5}'
  ```

## Data Ingestion
- Seed file `seed.jsonl` (24 comps/notes) ingested via helper:
  ```bash
  python scripts/ingest_jsonl.py /Users/hussain/Downloads/ValtricConsulting/seed.jsonl \
    --host http://127.0.0.1:8000
  ```
  - Script wraps JSONL lines into the required `deal+documents` shape, uses `text-embedding-3-small`, and posts to `/foundry/ingest`.
- Custom payloads can still be built manually, but must include `deal` and `documents` arrays plus an embedding.

## Retrieval & Analysis
- Latest `/analyze` for `deal_id=13` returns:
  ```json
  {
    "conclusion": "cheap",
    "comps_used": ["Q4_2024_Comps"],
    "risk_flags": [
      "Missing target operational metrics",
      "SMB-heavy comp risk (70% SMB mix)",
      "FX exposure (EUR, ~40%)",
      "Upsell concentration in top 15 accounts",
      "Unverified growth/retention metrics in teaser"
    ]
  }
  ```
  showing Supabase comps are injected.
- Adaptive reasoning: easy → GPT-5 Nano (minimal); hard → DeepSeek triage (low effort) + GPT-5 Nano (high effort). Latency logs confirm the two-stage cascade.

## Outstanding Work / Next Steps
1. **Evidence caching**
   - Populate `evidence_packs` with retrieved chunk ids (model/snapshot aware).
   - Update `/analyze` to read cached packs before hitting Supabase.
2. **Pipeline tracking**
   - Log each ingestion in `pipeline_runs`.
   - Capture embedding snapshots via `index_snapshots` if we re-embed.
3. **Front-end integration**
   - Build Foundry UI to upload documents, view ingestion status, and show retrieved evidence/risk summaries.
4. **Testing & Ops**
   - Add mocked Supabase/OpenAI tests for CI.
   - Document key rotation (Supabase/DeepSeek/OpenAI) and ensure `.env` never gets committed.
5. **Further ingestion tooling**
   - Convert shell loop shortcuts into CLI commands if we expect regular JSONL feeds.
   - Consider streaming ingestion progress/logging to avoid silent failures.

## Quick Command Reference
```bash
# Ingest JSONL
python scripts/ingest_jsonl.py ~/Downloads/ValtricConsulting/seed.jsonl --host http://127.0.0.1:8000

# Manual Supabase RPC check (after exporting service-role key)
curl -sS "https://wupfktkhluphvpfyqwcm.supabase.co/rest/v1/rpc/match_chunks" \
  -H "apikey: $SUPABASE_SERVICE_ROLE_KEY" \
  -H "Authorization: Bearer $SUPABASE_SERVICE_ROLE_KEY" \
  -H "Content-Type: application/json" \
  -H "Prefer: params=single-object" \
  --data '{"query_embedding":"[...1536 floats...]", "match_count":5}'

# Analyze a deal
curl -s -X POST http://127.0.0.1:8000/analyze \
  -H "Content-Type: application/json" \
  -d '{"deal_id":13,"question":"Compare this SaaS deal with manufacturing comps and highlight FX risks."}'
```

## Environment Notes
- Postgres via Docker (`docker-compose up -d db`) must be running before uvicorn.
- Supabase credentials live only in local `.env` and shell exports—rotate immediately if exposed.
- OpenAI/DeepSeek keys also stay in `.env` and shell; do not commit them.

## What’s Ready for Tomorrow
- Retrieval is hitting Supabase successfully and `/analyze` is evidence-aware.
- JSONL seeding pipeline is in place.
- `.env` document and README updates capture the new workflow.
- Next session can focus on caching, logging, or front-end work without redoing setup.
 Backend is healthy and fully wired into Supabase pgvector:

  - /foundry/ingest now accepts either full JSON payloads or JSONL via python scripts/ingest_jsonl.py ~/Downloads/ValtricConsulting/seed.jsonl --host
    http://127.0.0.1:8000; it generates text-embedding-3-small vectors, pushes deals/chunks/embeddings to Postgres + Supabase, and returns counts (24 records seeded in
    the latest run).
  - Supabase RPC match_chunks (public schema) works end-to-end with the service-role key (apikey/Authorization headers); Supabase tables now have chunk_id BIGINT and
    vectors at 1536 dims.
  - /analyze pulls real context—comps_used includes Q4_2024 comps, risk flags surface seeded details, and logs show supabase_vector_search timings with the adaptive
    DeepSeek → GPT-5 pipeline.

  What’s next:

  1. Add evidence caching (evidence_packs) so repeat analyses skip live vector search.
  2. Log ingestion runs via pipeline_runs / index_snapshots for governance/versioning.
  3. Begin front-end wiring (display retrieved evidence, upload status).
  4. Add mocked Supabase/OpenAI tests so CI doesn’t hit real services.